{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # [Apprentissage en Grande Dimension](https://github.com/wikistat/High-Dimensional-Learning ) : [Reconnaissance d'Activité Humaine](https://github.com/wikistat/High-Dimensional-Learning/tree/master/HumanActivityRecognition) ([*HAR*](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)) en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>  Seconde partie:  apprentissage (profond) des signaux bruts  avec <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a>\n",
    "\n",
    "Ce notebook présente la partie prediction de l'activité. Pour l'exploration, se référer au calepin afférent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Introduction\n",
    "##   Contexte\n",
    "Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*) à partir d’enregistrements, par exemple du gyroscope et de l'accéléromètre d'un smartphone.\n",
    "Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  \n",
    "\n",
    "Les données publiques disponibles ont été acquises, décrites et analysées par [Anguita et al. (2013)](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-84.pdf). Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine ainsi que sur le site *Kaggle*.\n",
    "\n",
    "L'archive contient les données brutes: accélérations en x, y, et z, chacun de 128 colonnes. D'autres fichiers en y soustrayant la gravité naturelle ainsi que les accélérations angulaires en x, y, et z, soit en tout 9 fichiers. Mais 6 utiles avec 6*128=768 mesures.\n",
    "\n",
    "Les méthodes d'apprentissage sont appliquées sur ces données brutes, sans calculs préliminaires de caractéristiques (*features*).\n",
    "\n",
    "##  Objectif\n",
    "Cette deuxième étape s'intéresse aux données brutes. Est-il possible d'économiser le travail préliminaire de définition des variables métier en utilisant, par exemple, un algorithme d'apprentissage profond sur les données brutes ou leur transformation en ondelettes ?\n",
    "\n",
    "**Objectif** Faire aussi bien (96% de bien classés) qu'avec les variables métier.\n",
    "\n",
    "##  Méthodes abordées : \n",
    "**Attention l'accès à un environnement *GPU* est très vivement conseillé voire indispensable.**\n",
    "- Modélisation, prévision de l'échantillon test par\n",
    "   - Régression logistique (`Scikit-learn`) sur signaux \"applatis\"\n",
    "   - Apprentissage profond en utilisant `Keras` \n",
    "       - MLP sur signaux \"applatis\"\n",
    "       - MLP sur signaux mutlidimensionels\n",
    "       - 1D Convolution\n",
    "       - 2D Convolution\n",
    "   - Validation Monte Carlo pour choisir le modèle\n",
    "   \n",
    "   \n",
    "- ** Extensions possibles  à ce travail: **\n",
    "\n",
    "    - Application des méthodes d'apprentissage classiques et de l'apprentissage profond sur les coefficients des décompositions des signaux en ondelettes\n",
    "    - Optimisation des paramètres des différentes méthodes.\n",
    "    - Améliorer l'architecture des réseaux ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Mise en place\n",
    "## Librairies et initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#Utils Sklearn\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "# DEEP LEARING\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# for reproducibility\n",
    "# https://github.com/fchollet/keras/issues/2280\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")\n",
    "\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras.models as km \n",
    "import keras.layers as kl \n",
    "import keras.layers.core as klc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "##  Prise en charge des données\n",
    "### Sources\n",
    "\n",
    "Les données sont celles originales du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle peuvent être téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Elles contiennent deux jeux de dimensions différentes, chacun partagé en apprentissage et test.\n",
    "\n",
    "1. Multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(N, 128, 9)$\n",
    "2. Unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(N, 1152)*\n",
    "        \n",
    "Deux objets différents sont construits pour définir la variable $Y$ réponse car les librairies `Scikit-learn` et `Keras` prennent en compte des structures différentes: \n",
    "    \n",
    "1. `Scikit-Learn`  Un vecteur de dimension $(N, 1)$ avec, pour chaque individu le numéro du label de l'activité de 0 à 5.\n",
    "2. `Keras` Une matrice de dimension $(N, 6)$ des indicatrices (0 ou 1) des modalités de $Y$.\n",
    "\n",
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attention: adapter si besoin le chemin d'accès aux données\n",
    "\n",
    "DATADIR_UCI = '.'\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = f'{data_dir}/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "    x = my_read_csv(filename).as_matrix()\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))\n",
    "        \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = f'{data_dir}/{subset}/y_{subset}.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    \n",
    "    \n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).as_matrix()\n",
    "    else:\n",
    "        Y = y.as_matrix()\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"WALKING\",\"WALKING UPSTAIRS\",\"WALKING DOWNSTAIRS\",\"SITTING\",\"STANDING\",\"LAYING\"]\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "\n",
    "def my_confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage des signaux uni-dimensionels\n",
    "\n",
    "La base d'apprentissage est de dimension (`N_train`, 1152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Régression Logistique\n",
    "\n",
    "La Régression Logistique est une des méthodes conduisant aux meilleurs résultats sur les variables métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_lr = lm.LogisticRegression(verbose=1)\n",
    "model_lr.fit(X_train_flatten, Y_train_label)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "score = model_lr.score(X_test_flatten, Y_test_label)\n",
    "print(\"Score With Logistic Regression on Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "lr_prediction_label = model_lr.predict(X_test_flatten)\n",
    "metadata_svc = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "pd.DataFrame(confusion_matrix(lr_prediction_label, Y_test_label), index = LABELS, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron multicouche\n",
    "\n",
    "Un réseau de neurones classique est appris sur les données au même format que précédemment.\n",
    "\n",
    "**Q** Expliciter les choix des paramètres et donc la structure du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "n_hidden = 32\n",
    "\n",
    "n_features = X_train_flatten.shape[1]\n",
    "n_classes=6\n",
    "\n",
    "\n",
    "model_base_mlp_u =km.Sequential()\n",
    "model_base_mlp_u.add(kl.Dense(n_hidden, input_shape=(n_features,),  activation = \"relu\"))\n",
    "model_base_mlp_u.add(kl.Dropout(0.5))\n",
    "model_base_mlp_u.add(kl.Dense(n_classes, activation='softmax'))\n",
    "model_base_mlp_u.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model_base_mlp_u.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp_u.fit(X_train_flatten,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test_flatten, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp_u.evaluate(X_test_flatten, Y_test_dummies)[1] \n",
    "print(\"Score With Simple MLP on Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp_u = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_u_prediction = model_base_mlp_u.predict(X_test_flatten)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_u_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q ** : Que conclure sur ces résultats en terme de performance, de temps d'apprentissage? Comparer avec la regression logistique?\n",
    "\n",
    "** Exercice ** : Quelle est l'influence de l'ajout de nouvelle couche? Supression du Dropout? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage des signaux multidimensionnels\n",
    "Les différents signaux ne sont pas concaténés en un seul signal mais pris en compte parallèlement.\n",
    "\n",
    "## Perceptron multichouche\n",
    "**Q** Expliciter les choix des paramètres et donc la structure du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 50\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "model_base_mlp =km.Sequential()\n",
    "model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"Score With Simple MLP on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_prediction = model_base_mlp.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau avec couche convolutionelle 1D (*ConvNet*)\n",
    "\n",
    "L'idée pertinente avec ces données est évidemment d'identifier le problème lié au déphasage des signaux. L'utilisation d'une couche convolutionnelle introduit une propriété d'invariance par translation. Les caractéristiques ou *features* sortant de cette couche acquièrent donc ainsi de bonnes propriétés avant d'être dirigées vers des couches techniques intermédiaires (`MaxPooling, Flatten`) et une dernière couche de sortie qui effectue la discrimination à partir des caractéristiques.\n",
    "\n",
    "**Q.** Remarquer le nombre de paramètres à estimer, le comparer avec celui du perceptron précédent et comprendre comment la convolution 1D agit sur les signaux (regarder en particulier la forme de la sortie de chaque couche du réseau). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "#else:\n",
    "model_base_conv_1D =km.Sequential()\n",
    "model_base_conv_1D.add(kl.Conv1D(32, 9, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "model_base_conv_1D.add(kl.MaxPooling1D(pool_size=3))\n",
    "model_base_conv_1D.add(kl.Flatten())\n",
    "model_base_conv_1D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "model_base_conv_1D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_conv_1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_conv_1D.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_conv_1D.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"Score With Conv on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_conv = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_conv_1D_prediction = model_base_conv_1D.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_conv_1D_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau avec couche convolutionelle 2D (*ConvNet*)\n",
    "\n",
    "**Q.** Remarquer le nombre de paramètres à estimer, le comparer avec celui du réseau précédent et comprendre comment la convolution 2D agit sur les signaux (regarder en particulier la forme de la sortie de chaque couche du réseau). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "X_train_conv = X_train.reshape(N_train, timesteps, input_dim, 1)\n",
    "X_test_conv = X_test.reshape(N_test, timesteps, input_dim, 1)\n",
    "\n",
    "#else:\n",
    "model_base_conv_2D =km.Sequential()\n",
    "model_base_conv_2D.add(kl.Conv2D(32, (3, 9), activation='relu', input_shape=(timesteps, input_dim, 1)))\n",
    "model_base_conv_2D.add(kl.MaxPooling2D(pool_size=(2, 1)))\n",
    "model_base_conv_2D.add(kl.Flatten())\n",
    "model_base_conv_2D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "model_base_conv_2D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_conv_2D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_conv_2D.fit(X_train_conv,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test_conv, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_conv_2D.evaluate(X_test_conv, Y_test_dummies)[1] \n",
    "print(\"Score With Conv on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_conv = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_conv_2D_prediction = model_base_conv_2D.predict(X_test_conv)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_conv_2D_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Attention au sur-apprentissage** A force de rechercher la meilleure architecture en minimisant l'erreur sur l'échantillon test, celle finalement trouvée peut y être très adaptée réduisant ainsi la capacité de généralisation. Il serait prudent de multiplier le découpage de l'échantillon par validation croisée *Monte Carlo*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de la Validation Croisée Monte Carlo\n",
    "\n",
    "**Objectif** :  trouver la meilleure architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.copy(np.concatenate((X_train, X_test), axis=0))\n",
    "Y=np.copy(np.concatenate((Y_train_dummies, Y_test_dummies), axis=0))\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "#Il faudrait en prendre plus ..\n",
    "batch_size = 32\n",
    "n_hidden = 32\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MC=10\n",
    "\n",
    "score=np.empty([N_MC,3])\n",
    "\n",
    "\n",
    "for k in range(N_MC):\n",
    "    print(\"\\n \\n *****************\",k,\"***************** \\n\")\n",
    "    X_train_MC,X_test_MC,Y_train_dummies_MC,Y_test_dummies_MC=train_test_split(X,Y,test_size=0.2)\n",
    "    N_train_MC = X_train_MC.shape[0]\n",
    "    N_test_MC = X_test_MC.shape[0]\n",
    "    \n",
    "    timesteps = len(X_train_MC[0])\n",
    "    input_dim = len(X_train_MC[0][0])\n",
    "    \n",
    "    X_train_conv_MC = X_train_MC.reshape(N_train_MC, timesteps, input_dim, 1)\n",
    "    X_test_conv_MC = X_test_MC.reshape(N_test_MC, timesteps, input_dim, 1)\n",
    "    \n",
    "    \n",
    "    # définition des modèles \n",
    "    model_base_mlp =km.Sequential()\n",
    "    model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "    model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "    model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "    model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model_base_conv_1D =km.Sequential()\n",
    "    model_base_conv_1D.add(kl.Conv1D(32, 9, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "    model_base_conv_1D.add(kl.MaxPooling1D(pool_size=3))\n",
    "    model_base_conv_1D.add(kl.Flatten())\n",
    "    model_base_conv_1D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "    model_base_conv_1D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    model_base_conv_2D =km.Sequential()\n",
    "    model_base_conv_2D.add(kl.Conv2D(32, (3, 9), activation='relu', input_shape=(timesteps, input_dim, 1)))\n",
    "    model_base_conv_2D.add(kl.MaxPooling2D(pool_size=(2, 1)))\n",
    "    model_base_conv_2D.add(kl.Flatten())\n",
    "    model_base_conv_2D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "    model_base_conv_2D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "   \n",
    "    print(\"\\n **** MLP **** \\n\")\n",
    "    model_base_mlp.fit(X_train_MC,  Y_train_dummies_MC, batch_size=batch_size, validation_data=(X_test_MC, Y_test_dummies_MC), epochs=epochs)   \n",
    "\n",
    "    print(\"\\n **** conv 1D **** \\n\")\n",
    "    model_base_conv_1D.fit(X_train_MC,  Y_train_dummies_MC, batch_size=batch_size, validation_data=(X_test_MC, Y_test_dummies_MC), epochs=epochs)\n",
    "   \n",
    "    print(\"\\n **** conv 2D **** \\n\")\n",
    "    model_base_conv_2D.fit(X_train_conv_MC,  Y_train_dummies_MC, batch_size=batch_size, validation_data=(X_test_conv_MC, Y_test_dummies_MC), epochs=epochs)\n",
    "    \n",
    "    score_mlp=model_base_mlp.evaluate(X_test_MC, Y_test_dummies_MC)[1]\n",
    "    #score_lstm=model_base_lstm.evaluate(X_test_MC, Y_test_dummies_MC)[1]\n",
    "    score_conv_1D=model_base_conv_1D.evaluate(X_test_MC, Y_test_dummies_MC)[1]\n",
    "    score_conv_2D=model_base_conv_2D.evaluate(X_test_conv_MC, Y_test_dummies_MC)[1]\n",
    "    s=[score_mlp,score_conv_1D,score_conv_2D]\n",
    "    score[k,:]=s\n",
    "    \n",
    "final_scores=np.apply_along_axis(np.mean,0,score)*100\n",
    "print(\"\\n\",score)\n",
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP     :\",round(final_scores[0],2),\"% \\n\")\n",
    "print(\"Conv 1D :\",round(final_scores[1],2),\"% \\n\")\n",
    "print(\"Conv 2D :\",round(final_scores[2],2),\"% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Quelle méthode est la plus performante ? Commenter ces résultats par rapport à ceux qui ont été obtenus sur les données \"métier\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
